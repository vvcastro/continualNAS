{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb13cc1b-ac98-4f60-8406-c22cec12b6bf",
   "metadata": {},
   "source": [
    "Implementation of the Net2Net primitives and how to look at the meaning of this expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23aa70e-42b5-4406-8f9e-ffb2519b3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45c7f0-8315-4305-9408-617fc3598a59",
   "metadata": {},
   "source": [
    "For out NAS we will mostly want to expand on top of pre-defined blocks (from the OFA set). So we are going to work on how to expand each of these kind of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f658a5cb-8444-4ca7-a20e-e49e5ea9d95e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' on line 1 (251872584.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    ])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '(' on line 1\n"
     ]
    }
   ],
   "source": [
    "torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "    torch.nn.BatchNorm2d(num_features=16),\n",
    "    torch.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0645e2-9663-4ef4-a9e9-f89295789d52",
   "metadata": {},
   "source": [
    "Wider operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61378756-29d1-4544-b2d2-f00fb06e4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wider(m1, m2, new_width, bnorm=None, out_size=None, noise=True,\n",
    "          random_init=True, weight_norm=True):\n",
    "    \"\"\"\n",
    "    Convert m1 layer to its wider version by adapthing next weight layer and\n",
    "    possible batch norm layer in btw.\n",
    "    Args:\n",
    "        m1 - module to be wider\n",
    "        m2 - follwing module to be adapted to m1\n",
    "        new_width - new width for m1.\n",
    "        bn (optional) - batch norm layer, if there is btw m1 and m2\n",
    "        out_size (list, optional) - necessary for m1 == conv3d and m2 == linear. It\n",
    "            is 3rd dim size of the output feature map of m1. Used to compute\n",
    "            the matching Linear layer size\n",
    "        noise (bool, True) - add a slight noise to break symmetry btw weights.\n",
    "        random_init (optional, True) - if True, new weights are initialized\n",
    "            randomly.\n",
    "        weight_norm (optional, True) - If True, weights are normalized before\n",
    "            transfering.\n",
    "    \"\"\"\n",
    "\n",
    "    w1 = m1.weight.data\n",
    "    w2 = m2.weight.data\n",
    "    b1 = m1.bias.data\n",
    "\n",
    "    if \"Conv\" in m1.__class__.__name__ or \"Linear\" in m1.__class__.__name__:\n",
    "        # Convert Linear layers to Conv if linear layer follows target layer\n",
    "        if \"Conv\" in m1.__class__.__name__ and \"Linear\" in m2.__class__.__name__:\n",
    "            assert w2.size(1) % w1.size(0) == 0, \"Linear units need to be multiple\"\n",
    "            if w1.dim() == 4:\n",
    "                factor = int(np.sqrt(w2.size(1) // w1.size(0)))\n",
    "                w2 = w2.view(w2.size(0), w2.size(1)//factor**2, factor, factor)\n",
    "            elif w1.dim() == 5:\n",
    "                assert out_size is not None,\\\n",
    "                       \"For conv3d -> linear out_size is necessary\"\n",
    "                factor = out_size[0] * out_size[1] * out_size[2]\n",
    "                w2 = w2.view(w2.size(0), w2.size(1)//factor, out_size[0],\n",
    "                             out_size[1], out_size[2])\n",
    "        else:\n",
    "            assert w1.size(0) == w2.size(1), \"Module weights are not compatible\"\n",
    "        assert new_width > w1.size(0), \"New size should be larger\"\n",
    "\n",
    "        old_width = w1.size(0)\n",
    "        nw1 = m1.weight.data.clone()\n",
    "        nw2 = w2.clone()\n",
    "\n",
    "        if nw1.dim() == 4:\n",
    "            nw1.resize_(new_width, nw1.size(1), nw1.size(2), nw1.size(3))\n",
    "            nw2.resize_(nw2.size(0), new_width, nw2.size(2), nw2.size(3))\n",
    "        elif nw1.dim() == 5:\n",
    "            nw1.resize_(new_width, nw1.size(1), nw1.size(2), nw1.size(3), nw1.size(4))\n",
    "            nw2.resize_(nw2.size(0), new_width, nw2.size(2), nw2.size(3), nw2.size(4))\n",
    "        else:\n",
    "            nw1.resize_(new_width, nw1.size(1))\n",
    "            nw2.resize_(nw2.size(0), new_width)\n",
    "\n",
    "        if b1 is not None:\n",
    "            nb1 = m1.bias.data.clone()\n",
    "            nb1.resize_(new_width)\n",
    "\n",
    "        if bnorm is not None:\n",
    "            nrunning_mean = bnorm.running_mean.clone().resize_(new_width)\n",
    "            nrunning_var = bnorm.running_var.clone().resize_(new_width)\n",
    "            if bnorm.affine:\n",
    "                nweight = bnorm.weight.data.clone().resize_(new_width)\n",
    "                nbias = bnorm.bias.data.clone().resize_(new_width)\n",
    "\n",
    "        w2 = w2.transpose(0, 1)\n",
    "        nw2 = nw2.transpose(0, 1)\n",
    "\n",
    "        nw1.narrow(0, 0, old_width).copy_(w1)\n",
    "        nw2.narrow(0, 0, old_width).copy_(w2)\n",
    "        nb1.narrow(0, 0, old_width).copy_(b1)\n",
    "\n",
    "        if bnorm is not None:\n",
    "            nrunning_var.narrow(0, 0, old_width).copy_(bnorm.running_var)\n",
    "            nrunning_mean.narrow(0, 0, old_width).copy_(bnorm.running_mean)\n",
    "            if bnorm.affine:\n",
    "                nweight.narrow(0, 0, old_width).copy_(bnorm.weight.data)\n",
    "                nbias.narrow(0, 0, old_width).copy_(bnorm.bias.data)\n",
    "\n",
    "        # TEST:normalize weights\n",
    "        if weight_norm:\n",
    "            for i in range(old_width):\n",
    "                norm = w1.select(0, i).norm()\n",
    "                w1.select(0, i).div_(norm)\n",
    "\n",
    "        # select weights randomly\n",
    "        tracking = dict()\n",
    "        for i in range(old_width, new_width):\n",
    "            idx = np.random.randint(0, old_width)\n",
    "            try:\n",
    "                tracking[idx].append(i)\n",
    "            except:\n",
    "                tracking[idx] = [idx]\n",
    "                tracking[idx].append(i)\n",
    "\n",
    "            # TEST:random init for new units\n",
    "            if random_init:\n",
    "                n = m1.kernel_size[0] * m1.kernel_size[1] * m1.out_channels\n",
    "                if m2.weight.dim() == 4:\n",
    "                    n2 = m2.kernel_size[0] * m2.kernel_size[1] * m2.out_channels\n",
    "                elif m2.weight.dim() == 5:\n",
    "                    n2 = m2.kernel_size[0] * m2.kernel_size[1] * m2.kernel_size[2] * m2.out_channels\n",
    "                elif m2.weight.dim() == 2:\n",
    "                    n2 = m2.out_features * m2.in_features\n",
    "                nw1.select(0, i).normal_(0, np.sqrt(2./n))\n",
    "                nw2.select(0, i).normal_(0, np.sqrt(2./n2))\n",
    "            else:\n",
    "                nw1.select(0, i).copy_(w1.select(0, idx).clone())\n",
    "                nw2.select(0, i).copy_(w2.select(0, idx).clone())\n",
    "            nb1[i] = b1[idx]\n",
    "\n",
    "        if bnorm is not None:\n",
    "            nrunning_mean[i] = bnorm.running_mean[idx]\n",
    "            nrunning_var[i] = bnorm.running_var[idx]\n",
    "            if bnorm.affine:\n",
    "                nweight[i] = bnorm.weight.data[idx]\n",
    "                nbias[i] = bnorm.bias.data[idx]\n",
    "            bnorm.num_features = new_width\n",
    "\n",
    "        if not random_init:\n",
    "            for idx, d in tracking.items():\n",
    "                for item in d:\n",
    "                    nw2[item].div_(len(d))\n",
    "\n",
    "        w2.transpose_(0, 1)\n",
    "        nw2.transpose_(0, 1)\n",
    "\n",
    "        m1.out_channels = new_width\n",
    "        m2.in_channels = new_width\n",
    "\n",
    "        if noise:\n",
    "            noise = np.random.normal(scale=5e-2 * nw1.std(),\n",
    "                                     size=list(nw1.size()))\n",
    "            nw1 += th.FloatTensor(noise).type_as(nw1)\n",
    "\n",
    "        m1.weight.data = nw1\n",
    "\n",
    "        if \"Conv\" in m1.__class__.__name__ and \"Linear\" in m2.__class__.__name__:\n",
    "            if w1.dim() == 4:\n",
    "                m2.weight.data = nw2.view(m2.weight.size(0), new_width*factor**2)\n",
    "                m2.in_features = new_width*factor**2\n",
    "            elif w2.dim() == 5:\n",
    "                m2.weight.data = nw2.view(m2.weight.size(0), new_width*factor)\n",
    "                m2.in_features = new_width*factor\n",
    "        else:\n",
    "            m2.weight.data = nw2\n",
    "\n",
    "        m1.bias.data = nb1\n",
    "\n",
    "        if bnorm is not None:\n",
    "            bnorm.running_var = nrunning_var\n",
    "            bnorm.running_mean = nrunning_mean\n",
    "            if bnorm.affine:\n",
    "                bnorm.weight.data = nweight\n",
    "                bnorm.bias.data = nbias\n",
    "        return m1, m2, bnorm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
